# -*- coding:utf-8 -*-import jsonimport timeimport pandas as pdimport requestsimport tracebackimport globimport os# change these parameters if needed# provide your own access_token and url here# access_token = ""access_token = ""base_url = "https://aip.baidubce.com/rpc/2.0/nlp/v2/comment_tag"charset = "UTF-8"# charset = "GBK2312"# input and output dir path, must exist on diskin_dir_path = 'D:\\tmp\\dir-wh'out_dir_path = 'D:\\tmp\\dir-wh-out'# response sample'''{    "log_id": 3911996551063569265,    "items": [        {            "sentiment": 2,            "abstract": "<span>老板人很好</span>",            "prop": "老板",            "begin_pos": 0,            "end_pos": 15,            "adj": "好"        },        {            "sentiment": 2,            "abstract": "没有太多游客也没有<span>浓厚的商业气息</span>",            "prop": "商业气息",            "begin_pos": 27,            "end_pos": 48,            "adj": "不浓厚"        }    ]}'''# sleep time between two invocations, unit: second# change it if neededsleep = 0.5class BaiduExamples(object):    def __init__(self):        self.charset = charset        self.access_token = access_token        # build url        self.build_url(base_url)        self.store_id_list = []        self.log_id_list = []        self.text_list = []        self.type_list = []        self.senti_list = []        self.abs_list = []        self.prop_list = []        self.begin_pros_list = []        self.end_pros_list = []        self.adj_list = []        self.df = None    # build post request url with parameters    def build_url(self, base_url):        self.word_analysis_url = base_url + "?charset=" + self.charset + "&access_token=" + self.access_token    # do analysis    # default: retry 3 times if failed, with interval of 3 seconds    def word_analysis(self, text, type, retries=3, interval=3):        # text = "这个冬天，如此惊艳，你要错过吗？这里，真的还蛮值得来的，自己来不用担心没人拍照，老板拍照技术一流，"        byte_size = len(text.encode('utf-8'))        datas = json.dumps({            "text": text,            "type": type        #}, ensure_ascii=False).encode('utf-8')        # }).encode('UTF-8')        }).encode(self.charset)        print("byte_size: %s, text: %s" % (byte_size, text))        code = -1        current = 0        res = None        while (code != 200 and current < retries):            res = requests.post(self.word_analysis_url, data=datas)            code = res.status_code            current += 1            if code != 200:                print("error, status_code is not 200, retries: %s/%s, interval: %s" % (current, retries, interval))                time.sleep(interval)        if code != 200:            print("error, status_code is not 200, something wrong with the post method, "                  "res.status_code: %s, res.text: %s, res.content: %s" % (res.status_code, res.text, res.content))            request = res.request            print("error, request.url: %s" % request.url)        res_str = res.content.decode()        return res_str    def append_item(self, store_id, log_id, text, type, senti, abs, prop, begin_pos, end_pos, adj):        # append into lists        self.store_id_list.append(store_id)        self.log_id_list.append(log_id)        self.text_list.append(text)        self.type_list.append(type)        self.senti_list.append(senti)        self.abs_list.append(abs)        self.prop_list.append(prop)        self.begin_pros_list.append(begin_pos)        self.end_pros_list.append(end_pos)        self.adj_list.append(adj)    # append those results parsed into lists    def append_list(self, store_id, text, type, res_text):        # parse json response        data = json.loads(res_text)        # parse parameters in json        log_id = data['log_id']        no_data = -999        items_size = len(data['items'])        if items_size == 0:            # when there is no data return in the response            senti = no_data            abs = no_data            prop = no_data            begin_pos = no_data            end_pos = no_data            adj = no_data            self.append_item(store_id, log_id, text, type, senti, abs, prop, begin_pos, end_pos, adj)        else:            for item in data['items']:                senti = item['sentiment']                abs = item['abstract']                prop = item['prop']                begin_pos = item['begin_pos']                end_pos = item['end_pos']                adj = item['adj']                self.append_item(store_id, log_id, text, type, senti, abs, prop, begin_pos, end_pos, adj)    # build pandas dataframe from lists    def build_dataframe(self):        self.df = pd.DataFrame(            {'store_id': self.store_id_list,             'log_id': self.log_id_list,             'text': self.text_list,             'type': self.type_list,             'sentiment': self.senti_list,             'abstract': self.abs_list,             'prop': self.prop_list,             'begin_pos': self.begin_pros_list,             'end_pos': self.end_pros_list,             'adj': self.adj_list             })    def save_df_to_excel(self, out_path):        self.df.to_excel(out_path, sheet_name='result')    def save_df_to_csv(self, out_path, sep=','):        self.df.to_csv(out_path, sep=',', encoding='utf-8')# read input xlsx filedef read_excel(in_filepath, out_dir):    # pd.set_option('display.float_format', lambda x: '%.2f' % x)    df = pd.read_excel(in_filepath)    df_li = df.values.tolist()    # rows and columns    row_count = len(df_li)    column_count = len(df_li[0])    print("row_count: %s, column_count: %s" % (row_count, column_count))    # build analysis_url    be = BaiduExamples()    be.build_url(base_url)    for x in range(row_count):        # store_id and text in column No.0 and column No.1, respectively        store_id = df_li[x][0]        text = df_li[x][1].strip()        type = df_li[x][2]        current = x + 1        print("%s/%s, store_id: %s" % (current, row_count, store_id))        # do analysis        res_text = None        try:            res_text = be.word_analysis(text, type)            # append results into lists            be.append_list(store_id, text, type, res_text)        except Exception as e:            print("error, %s/%s, store_id: %s, text: %s, res_text: %s" % (current, row_count, store_id, text, res_text))            traceback.print_exc()        # sleep seconds to wait for next api invocation        time.sleep(sleep)    # build pandas dataframe from all results lists    be.build_dataframe()    out_path = out_dir_path + "\\" + get_filename_without_extension(in_filepath) + "-result.xlsx"    # out_path_csv = out_dir_path + "\\" + get_filename_without_extension(in_filepath) + "-result.csv"    # save to xlsx and csv file    # print("save into xlsx file: %s, csv file: %s" % (out_path, out_path_csv))    print("save into xlsx file: %ss" % out_path)    be.save_df_to_excel(out_path)    # be.save_df_to_csv(out_path_csv)def list_files(dir, extension='\\*.xlsx'):    files = glob.glob(dir + extension)    return filesdef get_filename_without_extension(path):    basename = os.path.basename(path)    return os.path.splitext(basename)[0]if __name__ == "__main__":    # read_excel(in_path)    files = list_files(in_dir_path)    total = len(files)    current = 0    for f in files:        current += 1        print("%s/%s, file: %s" % (current, total, f))        read_excel(f, out_dir_path)