# -*- coding:utf-8 -*-import jsonimport timeimport pandas as pdimport requestsimport tracebackimport globimport os# change these parameters if needed# provide your own access_token and url here# access_token = ""access_token = ""base_url = "https://aip.baidubce.com/rpc/2.0/nlp/v1/lexer"charset = "UTF-8"# charset = "GBK2312"# input and output dir path, must exist on diskin_dir_path = 'D:\\tmp\\dir-wh-lexer'out_dir_path = 'D:\\tmp\\dir-wh-lexer-out'# response sample'''{    "log_id": 8987458173742824593,    "text": "三星电脑的电池不给力，垃圾。",    "items": [        {            "loc_details": [],            "byte_offset": 0,            "uri": "",            "pos": "",            "ne": "ORG",            "item": "三星",            "basic_words": [                "三星"            ],            "byte_length": 6,            "formal": ""        },        {            "loc_details": [],            "byte_offset": 6,            "uri": "",            "pos": "n",            "ne": "",            "item": "电脑",            "basic_words": [                "电脑"            ],            "byte_length": 6,            "formal": ""        }    ]}'''# sleep time between two invocations, unit: second# change it if neededsleep = 0.5class BaiduExamples(object):    def __init__(self):        self.charset = charset        self.access_token = access_token        # build url        self.build_url(base_url)        self.store_id_list = []        self.log_id_list = []        self.text_list = []        self.loc_details_list = []        self.byte_offset_list = []        self.uri_list = []        self.pos_list = []        self.ne_list = []        self.item_list = []        self.basic_words_list = []        self.byte_length_list = []        self.formal_list = []        self.df = None    # build post request url with parameters    def build_url(self, base_url):        self.word_analysis_url = base_url + "?charset=" + self.charset + "&access_token=" + self.access_token    # do analysis    # default: retry 3 times if failed, with interval of 3 seconds    def word_analysis(self, text, retries=3, interval=3):        # text = "这个冬天，如此惊艳，你要错过吗？这里，真的还蛮值得来的，自己来不用担心没人拍照，老板拍照技术一流，"        byte_size = len(text.encode('utf-8'))        datas = json.dumps({            "text": text            # }, ensure_ascii=False).encode('utf-8')            # }).encode('UTF-8')        }).encode(self.charset)        print("byte_size: %s, text: %s" % (byte_size, text))        code = -1        current = 0        res = None        while (code != 200 and current < retries):            res = requests.post(self.word_analysis_url, data=datas)            code = res.status_code            current += 1            if code != 200:                print("error, status_code is not 200, retries: %s/%s, interval: %s" % (current, retries, interval))                time.sleep(interval)        if code != 200:            print("error, status_code is not 200, something wrong with the post method, "                  "res.status_code: %s, res.text: %s, res.content: %s" % (res.status_code, res.text, res.content))            request = res.request            print("error, request.url: %s" % request.url)        res_str = res.content.decode()        return res_str    def append_item(self, store_id, log_id, text,                    loc_details, byte_offset, uri, pos, ne,                    item, basic_words, byte_length, formal):        # append into lists        self.store_id_list.append(store_id)        self.log_id_list.append(log_id)        self.text_list.append(text)        self.loc_details_list.append(loc_details)        self.byte_offset_list.append(byte_offset)        self.uri_list.append(uri)        self.pos_list.append(pos)        self.ne_list.append(ne)        self.item_list.append(item)        self.basic_words_list.append(basic_words)        self.byte_length_list.append(byte_length)        self.formal_list.append(formal)    # append those results parsed into lists    def append_list(self, store_id, text, res_text):        # parse json response        data = json.loads(res_text)        # parse parameters in json        log_id = data['log_id']        no_data = -999        items_size = len(data['items'])        if items_size == 0:            loc_details = no_data            byte_offset = no_data            uri = no_data            pos = no_data            ne = no_data            item = no_data            basic_words = no_data            byte_length = no_data            formal = no_data            self.append_item(store_id, log_id, text,                             loc_details, byte_offset, uri, pos, ne,                             item, basic_words, byte_length, formal)        else:            for item_obj in data['items']:                loc_details = item_obj['loc_details']                byte_offset = item_obj['byte_offset']                uri = item_obj['uri']                pos = item_obj['pos']                ne = item_obj['ne']                item = item_obj['item']                basic_words = item_obj['basic_words']                byte_length = item_obj['byte_length']                formal = item_obj['formal']                self.append_item(store_id, log_id, text,                                 loc_details, byte_offset, uri, pos, ne,                                 item, basic_words, byte_length, formal)    # build pandas dataframe from lists    def build_dataframe(self):        self.df = pd.DataFrame(            {'store_id': self.store_id_list,             'log_id': self.log_id_list,             'text': self.text_list,             'loc_details': self.loc_details_list,             'byte_offset': self.byte_offset_list,             'uri': self.uri_list,             'pos': self.pos_list,             'ne': self.ne_list,             'item': self.item_list,             'basic_words': self.basic_words_list,             'byte_length': self.byte_length_list,             'formal': self.formal_list             })    def exist_dataframe(self):        return hasattr(self, 'df')    def free_dataframe(self):        del self.df    def clear_lists(self):        self.store_id_list.clear()        self.log_id_list.clear()        self.text_list.clear()        self.loc_details_list.clear()        self.byte_offset_list.clear()        self.uri_list.clear()        self.pos_list.clear()        self.ne_list.clear()        self.item_list.clear()        self.basic_words_list.clear()        self.byte_length_list.clear()        self.formal_list.clear()    def save_df_to_excel(self, out_path, sheet_name):        self.df.to_excel(out_path, sheet_name)    def save_df_to_csv(self, out_path, sep=','):        self.df.to_csv(out_path, sep=',', encoding='utf-8')# batch interval to process, then save to excel filebatch_interval = 100# read input xlsx filedef read_excel(in_filepath, out_dir):    # pd.set_option('display.float_format', lambda x: '%.2f' % x)    df = pd.read_excel(in_filepath)    df_li = df.values.tolist()    # rows and columns    row_count = len(df_li)    column_count = len(df_li[0])    print("row_count: %s, column_count: %s" % (row_count, column_count))    # build analysis_url    be = BaiduExamples()    be.build_url(base_url)    last = 1    current = 0    batch_invoked = False    for x in range(row_count):        batch_invoked = False        # store_id and text in column No.0 and column No.1, respectively        store_id = df_li[x][0]        text = df_li[x][1].strip()        current = x + 1        print("%s/%s, store_id: %s" % (current, row_count, store_id))        # do analysis        res_text = None        try:            res_text = be.word_analysis(text)            # append results into lists            be.append_list(store_id, text, res_text)        except Exception as e:            print("error, %s/%s, store_id: %s, text: %s, res_text: %s" % (current, row_count, store_id, text, res_text))            traceback.print_exc()        # sleep seconds to wait for next api invocation        time.sleep(sleep)        sheet_name = "{}-{}".format(last, current)        if current % batch_interval == 0:            batch_invoked = True            # build pandas dataframe from all results lists            be.build_dataframe()            out_path = out_dir_path + "\\" + get_filename_without_extension(in_filepath) + "-result-{}.xlsx".format(                sheet_name)            # save to xlsx and csv file            print("save into xlsx file: %s, sheet_name: %s" % (out_path, sheet_name))            be.save_df_to_excel(out_path, sheet_name)            # clear current lists and free memory            print("clear current lists and free memory")            be.clear_lists()            be.free_dataframe()            last = current + 1    # save the remaining records that not reaching the batch interval    sheet_name = "{}-{}".format(last, current)    if not batch_invoked:        # build pandas dataframe from all results lists        be.build_dataframe()        out_path = out_dir_path + "\\" + get_filename_without_extension(in_filepath) + "-result-{}.xlsx".format(            sheet_name)        # save to xlsx and csv file        print("save into xlsx file: %s, sheet_name: %s" % (out_path, sheet_name))        be.save_df_to_excel(out_path, sheet_name)        # clear current lists and free memory        print("clear current lists and free memory")        be.clear_lists()        be.free_dataframe()def list_files(dir, extension='\\*.xlsx'):    files = glob.glob(dir + extension)    return filesdef get_filename_without_extension(path):    basename = os.path.basename(path)    return os.path.splitext(basename)[0]if __name__ == "__main__":    # read_excel(in_path)    files = list_files(in_dir_path)    total = len(files)    current = 0    for f in files:        current += 1        print("%s/%s, file: %s" % (current, total, f))        read_excel(f, out_dir_path)